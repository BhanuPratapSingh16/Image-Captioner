{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BhanuPratapSingh16/Image-Captioner/blob/main/Image_Captioner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfhZIgmXPXMw"
      },
      "source": [
        "# Dataset filtering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfZe4TJultxR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jl-mSxLm-hf",
        "outputId": "78056b5a-b5a5-472e-f63a-c56e773914c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2025.8.3)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.4.3)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "2wm_EG90V-wv",
        "outputId": "a9056ec8-6be3-4a5b-bfa1-6979feee2189"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-076b8412-68f7-4035-aa69-ea351e7f1823\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-076b8412-68f7-4035-aa69-ea351e7f1823\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"kakarot167vegeta\",\"key\":\"f7ed2086f7177f1009a3581f363ee6af\"}'}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rg2rjq_wWI_E"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "899RqHPSXYYD"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d adityajn105/flickr8k\n",
        "!unzip flickr8k.zip -d flickr8k\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtkUravIXzgc",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "image_captions = {}\n",
        "with open('/content/flickr8k/captions.txt', 'r') as f:\n",
        "    line = f.readline()\n",
        "    while line:\n",
        "        image, caption = line.split('.jpg,')\n",
        "        image = image + '.jpg'\n",
        "        caption = caption.strip().lower()[:-1].strip()\n",
        "        cleaned = caption.translate(str.maketrans('', '', string.punctuation))\n",
        "        caption = cleaned.strip()\n",
        "        caption = '<start>' + caption + '<end>'\n",
        "\n",
        "        if image not in image_captions.keys():\n",
        "            image_captions[image] = []\n",
        "        image_captions[image].append(caption)\n",
        "        line = f.readline()\n",
        "\n",
        "print(image_captions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKne1OywZZu5"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# save\n",
        "with open(\"image_captions.json\", \"w\") as f:\n",
        "    json.dump(image_captions, f)\n",
        "\n",
        "# load\n",
        "with open(\"image_captions.json\", \"r\") as f:\n",
        "    image_captions = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azSIYGTBdV2E",
        "outputId": "4f88dfba-8dce-4e4b-f8e8-3dc028dab5ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgcIdxetd4kc"
      },
      "outputs": [],
      "source": [
        "!mv /content/flickr8k /content/drive/MyDrive/datasets/flickr8k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcZcWaaJeKaJ"
      },
      "outputs": [],
      "source": [
        "!mv /content/image_captions.json /content/drive/MyDrive/datasets/flickr8k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toJyHpE8gAgT"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "images = list(image_captions.keys())\n",
        "random.shuffle(images)\n",
        "print(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEnQM0t6l9-B",
        "outputId": "5c8ef614-0289-4574-dfff-b6a36fbcbb63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8091\n"
          ]
        }
      ],
      "source": [
        "num_images = len(images)\n",
        "print(num_images)\n",
        "train_size = int(0.8 * num_images)\n",
        "val_size = int(0.1 * num_images)\n",
        "test_size = num_images - train_size - val_size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xP9wfSLGnzI0"
      },
      "outputs": [],
      "source": [
        "train_images, val_images, test_images = images[:train_size], images[train_size:train_size+val_size], images[train_size+val_size:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdILw78In-_Q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "base_dir = \"/content/drive/MyDrive/datasets/flickr8k/flickr8k/\"\n",
        "for split in [\"train\", \"val\", \"test\"]:\n",
        "        os.makedirs(os.path.join(base_dir, split), exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXzjGgGdqKkp"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "original_images_dir = \"/content/drive/MyDrive/datasets/flickr8k/flickr8k/Images\"\n",
        "\n",
        "# Function to copy images to split folder\n",
        "def copy_images(image_list, split_name):\n",
        "        for img_name in image_list:\n",
        "                src = os.path.join(original_images_dir, img_name)\n",
        "                dst = os.path.join(base_dir, split_name, img_name)\n",
        "                shutil.copy(src, dst)\n",
        "\n",
        "# Copy images\n",
        "copy_images(train_images, \"train\")\n",
        "copy_images(val_images, \"val\")\n",
        "copy_images(test_images, \"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaVpjWdEtA5O",
        "outputId": "2bdd42ce-d0b4-4500-ca87-efb484a1f2cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6472\n",
            "809\n",
            "810\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "base_dir = \"/content/drive/MyDrive/datasets/flickr8k/flickr8k/\"\n",
        "\n",
        "print(len(os.listdir(os.path.join(base_dir, \"train\"))))\n",
        "print(len(os.listdir(os.path.join(base_dir, \"val\"))))\n",
        "print(len(os.listdir(os.path.join(base_dir, \"test\"))))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3msKOfeuqfx"
      },
      "outputs": [],
      "source": [
        "!mv /content/drive/MyDrive/datasets/flickr8k/flickr8k/test /content/drive/MyDrive/datasets/flickr8k/flickr8k/split/test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PW88T0IqJr_r"
      },
      "outputs": [],
      "source": [
        "!mv /content/drive/MyDrive/datasets/flickr8k/flickr8k/train /content/drive/MyDrive/datasets/flickr8k/flickr8k/split/train\n",
        "!mv /content/drive/MyDrive/datasets/flickr8k/flickr8k/val /content/drive/MyDrive/datasets/flickr8k/flickr8k/split/val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXLmQIurJ3n4"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open(\"/content/drive/MyDrive/datasets/flickr8k/image_captions.json\", \"r\") as f:\n",
        "    image_captions = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxTaI8anLVvA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "train_images = os.listdir(os.path.join(base_dir, \"train\"))\n",
        "val_images     = os.listdir(os.path.join(base_dir, \"val\"))\n",
        "test_images    = os.listdir(os.path.join(base_dir, \"test\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDGR0FiELpQ0"
      },
      "outputs": [],
      "source": [
        "train_captions = {img: image_captions[img] for img in train_images if img in image_captions}\n",
        "val_captions     = {img: image_captions[img] for img in val_images if img in image_captions}\n",
        "test_captions    = {img: image_captions[img] for img in test_images if img in image_captions}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7XXTPN8Mhk7",
        "outputId": "6a7e1c8e-155f-48fa-a24c-b98854e3ed8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6472\n",
            "809\n",
            "810\n"
          ]
        }
      ],
      "source": [
        "print(len(train_captions))\n",
        "print(len(val_captions))\n",
        "print(len(test_captions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_eJtSymMpB5"
      },
      "outputs": [],
      "source": [
        "with open(base_dir+\"/train_captions.json\", \"w\") as f:\n",
        "        json.dump(train_captions, f)\n",
        "\n",
        "with open(base_dir+\"/val_captions.json\", \"w\") as f:\n",
        "        json.dump(val_captions, f)\n",
        "\n",
        "with open(base_dir+\"/test_captions.json\", \"w\") as f:\n",
        "        json.dump(test_captions, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnlUzoo0PLmZ"
      },
      "source": [
        "# Dataset preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pC54LqINb-V"
      },
      "outputs": [],
      "source": [
        "base_dir = \"/content/drive/MyDrive/datasets/flickr8k/flickr8k/split\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3TLalLu1mRk"
      },
      "source": [
        "### Building vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywDRP7aJzhaE"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from collections import Counter\n",
        "\n",
        "def build_vocab(captions_file, threshold):\n",
        "    with open(captions_file, \"r\") as f:\n",
        "        captions = json.load(f)\n",
        "\n",
        "    counter = Counter()\n",
        "    for captions in captions.values():\n",
        "        for caption in captions:\n",
        "            tokens = caption.split()\n",
        "            counter.update(tokens)\n",
        "\n",
        "    special_tokens = [\"<pad>\", \"<start>\", \"<end>\", \"<unk>\"]\n",
        "    word2idx = {token:idx for idx, token in enumerate(special_tokens)}\n",
        "    idx2word = {idx:token for idx, token in enumerate(special_tokens)}\n",
        "\n",
        "    del counter[\"<start>\"]\n",
        "    del counter[\"<end>\"]\n",
        "\n",
        "    for token, count in counter.items():\n",
        "        if count >= threshold:\n",
        "            idx = len(word2idx)\n",
        "            word2idx[token] = idx\n",
        "            idx2word[idx] = token\n",
        "\n",
        "    print(f\"Vocabulary size: {len(word2idx)}\")\n",
        "    return word2idx, idx2word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJ0bYcZ427P_",
        "outputId": "04e60b3f-9d10-4b9d-9623-879eea591c7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 5032\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "train_captions_file = os.path.join(base_dir, \"train_captions.json\")\n",
        "word2idx, idx2word = build_vocab(train_captions_file, threshold=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAXdsFFf31Mg"
      },
      "outputs": [],
      "source": [
        "print(word2idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joEASzi74SIO"
      },
      "outputs": [],
      "source": [
        "print(idx2word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cgtu1g4oprM3",
        "outputId": "49643a5d-73af-4280-ee10-b9b8ad897050"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-09-30 05:28:20--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2025-09-30 05:28:20--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2025-09-30 05:28:20--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.00MB/s    in 2m 40s  \n",
            "\n",
            "2025-09-30 05:31:00 (5.15 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove/glove.6B.50d.txt  \n",
            "  inflating: glove/glove.6B.100d.txt  \n",
            "  inflating: glove/glove.6B.200d.txt  \n",
            "  inflating: glove/glove.6B.300d.txt  \n"
          ]
        }
      ],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip -d glove"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rx8FT0cEE05D"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def load_glove(path):\n",
        "    embeddings = {}\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            parts = line.split()\n",
        "            word = parts[0]\n",
        "            vector = np.array(parts[1:], dtype=np.float32)\n",
        "            embeddings[word] = vector\n",
        "    return embeddings\n",
        "\n",
        "glove_embeddings = load_glove(\"/content/drive/MyDrive/datasets/flickr8k/flickr8k/glove/glove.6B.100d.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzOINXiA5trv"
      },
      "source": [
        "### Building embedding matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dThtztFV4Z7O"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 100\n",
        "vocab_size = len(word2idx)\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "for word, idx in word2idx.items():\n",
        "    if word in glove_embeddings:\n",
        "        embedding_matrix[idx] = glove_embeddings[word]\n",
        "    else:\n",
        "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0pYCsnc6R5y"
      },
      "outputs": [],
      "source": [
        "\n",
        "embedding_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.save(\"/content/drive/MyDrive/datasets/flickr8k/flickr8k/embedding_matrix.npy\", embedding_matrix)"
      ],
      "metadata": {
        "id": "ZMCTe7GGhMIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQJ7paEY6dHy"
      },
      "outputs": [],
      "source": [
        "def caption_to_seq(caption, word2idx, max_length):\n",
        "    words = caption.split()\n",
        "    seq = []\n",
        "\n",
        "    for w in words:\n",
        "        seq.append(word2idx.get(w, word2idx[\"<unk>\"]))\n",
        "\n",
        "    while(len(seq) < max_length):\n",
        "        seq.append(word2idx[\"<pad>\"])\n",
        "\n",
        "    if(len(seq) > max_length):\n",
        "        seq = seq[:max_length]\n",
        "\n",
        "    return seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIRTXfZnGVMs"
      },
      "outputs": [],
      "source": [
        "max_length = 20\n",
        "\n",
        "train_sequences = {}\n",
        "for img_name, captions in train_captions.items():\n",
        "    train_sequences[img_name] = [caption_to_seq(c, word2idx, max_length) for c in captions]\n",
        "\n",
        "test_sequences = {}\n",
        "for img_name, captions in test_captions.items():\n",
        "    test_sequences[img_name] = [caption_to_seq(c, word2idx, max_length) for c in captions]\n",
        "\n",
        "val_sequences = {}\n",
        "for img_name, captions in val_captions.items():\n",
        "    val_sequences[img_name] = [caption_to_seq(c, word2idx, max_length) for c in captions]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7elGieHNJOPv",
        "outputId": "8dbadf97-e16e-45bf-9217-6f495201c2ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6472\n",
            "810\n",
            "809\n"
          ]
        }
      ],
      "source": [
        "print(len(train_sequences))\n",
        "print(len(test_sequences))\n",
        "print(len(val_sequences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "P-Ji5Ds0Lj2A",
        "outputId": "329f33b4-d6f7-43c2-85b5-b33e6d02a4e5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/datasets/flickr8k/flickr8k/split'"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLcESU3ILNdT"
      },
      "outputs": [],
      "source": [
        "with open(base_dir+\"/train_sequences.json\", \"w\") as f:\n",
        "        json.dump(train_sequences, f)\n",
        "\n",
        "with open(base_dir+\"/val_sequences.json\", \"w\") as f:\n",
        "        json.dump(val_sequences, f)\n",
        "\n",
        "with open(base_dir+\"/test_sequences.json\", \"w\") as f:\n",
        "        json.dump(test_sequences, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YRKV7_PLshX",
        "outputId": "e3c05019-d88c-4dbb-ab51-0f49bb275fd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 8, 13, 6, 4, 14, 2, 0, 0, 0, 0], [1, 4, 5, 6, 15, 16, 17, 18, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 19, 15, 11, 8, 20, 21, 22, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 19, 15, 8, 23, 24, 25, 26, 11, 8, 4, 27, 28, 2, 0, 0, 0, 0, 0, 0], [1, 19, 29, 7, 11, 30, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
          ]
        }
      ],
      "source": [
        "print(train_sequences[\"3694093650_547259731e.jpg\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKLzsNuLNcGu"
      },
      "source": [
        "# CNN Encoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir(os.path.join(base_dir,\"train\")))"
      ],
      "metadata": {
        "id": "PREq4XVZfCmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torch"
      ],
      "metadata": {
        "id": "SWQ4EJRh5BZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, img_dir, transform=None):\n",
        "        self.dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.img_list = os.listdir(self.dir)\n",
        "\n",
        "        self.data = []\n",
        "        for img_name in self.img_list:\n",
        "            img_path = os.path.join(img_dir, img_name)\n",
        "            self.data.append(img_path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = self.data[index]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, img_path"
      ],
      "metadata": {
        "id": "JfeUhOYLeliG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "U3-vuXdtf41M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ImageDataset(\n",
        "        img_dir = os.path.join(base_dir, \"train\"),\n",
        "        transform = transform\n",
        ")\n",
        "\n",
        "val_dataset = ImageDataset(\n",
        "        img_dir = os.path.join(base_dir, \"val\"),\n",
        "        transform = transform\n",
        ")\n",
        "\n",
        "test_dataset = ImageDataset(\n",
        "        img_dir = os.path.join(base_dir, \"test\"),\n",
        "        transform = transform\n",
        ")\n",
        "\n",
        "# Loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "RjNq7F_rf_XZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxT08WhggL7S",
        "outputId": "bf614c5b-618f-4c39-9d3b-7ce889b17c76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<torch.utils.data.dataloader.DataLoader object at 0x7c4ef8d8b140>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-62SvSAMXg5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "PNJ_xO78YgMy"
      },
      "outputs": [],
      "source": [
        "model = models.resnet50(pretrained=True)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "model = nn.Sequential(*list(model.children())[:-1])\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFx60U-T5f2w",
        "outputId": "f2beb860-594b-4396-ad5c-9b3efce9b5b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLNUCf6U_RO5"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def extract_features(dataloader, model, device):\n",
        "    model.eval()\n",
        "    features = {}\n",
        "    with torch.no_grad():\n",
        "        for imgs, img_paths in tqdm(dataloader):\n",
        "            imgs = imgs.to(device)\n",
        "            outputs = model(imgs)\n",
        "            outputs = outputs.squeeze()\n",
        "\n",
        "            for i, path in enumerate(img_paths):\n",
        "                features[path] = outputs[i].cpu().numpy()\n",
        "    return features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvXQuSoLY_GV",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "train_features = extract_features(train_loader, model, device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"/content/drive/MyDrive/datasets/flickr8k/flickr8k/split/train_features.pkl\", \"wb\") as f:\n",
        "        pickle.dump(train_features, f)"
      ],
      "metadata": {
        "id": "1gHPumgVEu2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_features = extract_features(val_loader, model, device)\n",
        "with open(\"/content/drive/MyDrive/datasets/flickr8k/flickr8k/split/val_features.pkl\", \"wb\") as f:\n",
        "        pickle.dump(val_features, f)"
      ],
      "metadata": {
        "id": "pnkJHMAA6U5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_features = extract_features(test_loader, model, device)\n",
        "with open(\"/content/drive/MyDrive/datasets/flickr8k/flickr8k/split/test_features.pkl\", \"wb\") as f:\n",
        "        pickle.dump(test_features, f)"
      ],
      "metadata": {
        "id": "UfMaoLMa68E7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Decoder"
      ],
      "metadata": {
        "id": "qZg5fufkZGNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = \"/content/drive/MyDrive/datasets/flickr8k/flickr8k/split\""
      ],
      "metadata": {
        "id": "3oG87_1tZH_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "with open(os.path.join(base_dir, \"train_sequences.json\"), \"r\") as f:\n",
        "    train_sequences = json.load(f)\n",
        "\n",
        "with open(os.path.join(base_dir, \"val_sequences.json\"), \"r\") as f:\n",
        "    val_sequences = json.load(f)\n",
        "\n",
        "with open(os.path.join(base_dir, \"test_sequences.json\"), \"r\") as f:\n",
        "    test_sequences = json.load(f)"
      ],
      "metadata": {
        "id": "G7g0eWZvK97n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open(os.path.join(base_dir, \"train_features.pkl\"), \"rb\") as f:\n",
        "    train_features = pickle.load(f)\n",
        "\n",
        "with open(os.path.join(base_dir, \"val_features.pkl\"), \"rb\") as f:\n",
        "    val_features = pickle.load(f)\n",
        "\n",
        "with open(os.path.join(base_dir, \"test_features.pkl\"), \"rb\") as f:\n",
        "    test_features = pickle.load(f)"
      ],
      "metadata": {
        "id": "BjxSBWblLRAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "Ju-1svcMMCmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CaptionDataset(Dataset):\n",
        "    def __init__(self, sequences, features):\n",
        "        self.sequences = sequences\n",
        "        self.features = features\n",
        "\n",
        "        self.data = []\n",
        "        for img, seq in self.sequences.items():\n",
        "            for s in seq:\n",
        "                self.data.append((img, s))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_name, seqs = self.data[index]\n",
        "        feature = self.features[img_name]\n",
        "        return feature, seqs"
      ],
      "metadata": {
        "id": "1LkQG_dzLYbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CaptionDataset(\n",
        "        sequences = train_sequences,\n",
        "        features = train_features\n",
        ")\n",
        "\n",
        "val_dataset = CaptionDataset(\n",
        "        sequences = val_sequences,\n",
        "        features = val_features\n",
        ")\n",
        "\n",
        "test_dataset = CaptionDataset(\n",
        "        sequences = test_sequences,\n",
        "        features = test_features\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "aEcEiapoSc57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_sequences))\n",
        "print(len(train_features))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHP9Ts81rDyu",
        "outputId": "6c8fe0d8-cc2f-4bbd-cb9c-e9846626d8da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6472\n",
            "6472\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_features[\"3039675864_0b7961844d.jpg\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78rgSItG3sKo",
        "outputId": "35a4a0e8-8f27-4c87-8b9f-5259903d0c33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.25472474, 1.0270143 , 0.23229678, ..., 0.5405816 , 0.9122872 ,\n",
              "       0.54372734], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_sequences[\"3039675864_0b7961844d.jpg\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3S4La4kHTHbD",
        "outputId": "6c2b7f64-3537-432d-81f7-c1a2b9f54b2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 4, 122, 44, 4, 1076, 209, 19, 268, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [1,\n",
              "  4,\n",
              "  105,\n",
              "  257,\n",
              "  4,\n",
              "  3,\n",
              "  33,\n",
              "  242,\n",
              "  1154,\n",
              "  50,\n",
              "  4,\n",
              "  1188,\n",
              "  1005,\n",
              "  120,\n",
              "  2420,\n",
              "  2,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " [1, 4, 261, 40, 2420, 4, 1094, 24, 4, 165, 50, 4, 1188, 2, 0, 0, 0, 0, 0, 0],\n",
              " [1, 19, 1123, 11, 8, 19, 261, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [1,\n",
              "  241,\n",
              "  165,\n",
              "  133,\n",
              "  68,\n",
              "  4,\n",
              "  972,\n",
              "  40,\n",
              "  4,\n",
              "  880,\n",
              "  6,\n",
              "  2420,\n",
              "  50,\n",
              "  19,\n",
              "  261,\n",
              "  58,\n",
              "  59,\n",
              "  19,\n",
              "  2,\n",
              "  0]]"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cupy as cp\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "pW6WYzjXBaj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMDecoder:\n",
        "    def __init__(self, embedding_dim, hidden_dim, img_dim, vocab_size, embedding_matrix, lr):\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.input_dim = embedding_dim + hidden_dim\n",
        "        self.embedding_matrix = cp.array(embedding_matrix)\n",
        "        self.img_dim = img_dim\n",
        "        self.lr = lr\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # Adam Optimizer parameters\n",
        "        self.beta_1 = 0.9\n",
        "        self.beta_2 = 0.999\n",
        "        self.epsilon = 1e-8\n",
        "        self.m = {}\n",
        "        self.v = {}\n",
        "        self.t = 0\n",
        "\n",
        "\n",
        "        # CNN features to initial hidden state\n",
        "        self.W_h = cp.random.randn(self.img_dim, self.hidden_dim) * 0.01\n",
        "        self.b_h = cp.zeros((self.hidden_dim,))\n",
        "\n",
        "        # CNN features to initial cell state\n",
        "        self.W_c = cp.random.randn(self.img_dim, self.hidden_dim) * 0.01\n",
        "        self.b_c = cp.zeros((self.hidden_dim,))\n",
        "\n",
        "        # LSTM parameters\n",
        "        # Forget Gate\n",
        "        self.W_f = cp.random.randn(self.input_dim, self.hidden_dim) * 0.01\n",
        "        self.b_f = cp.ones((self.hidden_dim,))\n",
        "\n",
        "        # Input Gate\n",
        "        self.W_i = cp.random.randn(self.input_dim, self.hidden_dim) * 0.01\n",
        "        self.b_i = cp.zeros((self.hidden_dim,))\n",
        "\n",
        "        # Candidate cell\n",
        "        self.W_cand = cp.random.randn(self.input_dim, self.hidden_dim) * 0.01\n",
        "        self.b_cand = cp.zeros((self.hidden_dim,))\n",
        "\n",
        "        # Output Gate\n",
        "        self.W_o = cp.random.randn(self.input_dim, self.hidden_dim) * 0.01\n",
        "        self.b_o = cp.zeros((self.hidden_dim,))\n",
        "\n",
        "        # Output projections\n",
        "        self.W_out = cp.random.randn(self.hidden_dim, vocab_size) * 0.01\n",
        "        self.b_out = cp.zeros((vocab_size,))\n",
        "\n",
        "        self.initialize_adam()\n",
        "\n",
        "    def initialize_adam(self):\n",
        "        for name in ['embedding_matrix', 'W_h', 'b_h', 'W_c', 'b_c',\n",
        "                     'W_f', 'b_f', 'W_i', 'b_i', 'W_cand', 'b_cand',\n",
        "                     'W_o', 'b_o', 'W_out', 'b_out']:\n",
        "            weight = getattr(self, name)\n",
        "            self.m[name] = cp.zeros_like(weight)\n",
        "            self.v[name] = cp.zeros_like(weight)\n",
        "\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return cp.where(x >= 0,\n",
        "                1 / (1 + cp.exp(-x)),\n",
        "                cp.exp(x) / (1 + cp.exp(x)))\n",
        "\n",
        "    def tanh(self, x):\n",
        "        return cp.tanh(x)\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exp_x = cp.exp(x - cp.max(x, axis=-1, keepdims=True))\n",
        "        return exp_x / cp.sum(exp_x, axis=-1, keepdims=True)\n",
        "\n",
        "    def forward_step(self, x_batch, h_prev_batch, c_prev_batch):\n",
        "        batch_size = x_batch.shape[0]\n",
        "        hidden_size = h_prev_batch.shape[1]\n",
        "\n",
        "        # Concatenate input and previous hidden states\n",
        "        concat = cp.concatenate([x_batch, h_prev_batch], axis=1)  # (batch_size, input_dim)\n",
        "\n",
        "        # Compute gates\n",
        "        f = self.sigmoid(concat @ self.W_f + self.b_f)   # (batch_size, hidden size)\n",
        "        i = self.sigmoid(concat @ self.W_i + self.b_i)\n",
        "        cand_c = self.tanh(concat @ self.W_cand + self.b_cand)\n",
        "        o = self.sigmoid(concat @ self.W_o + self.b_o)\n",
        "\n",
        "        # Update cell state\n",
        "        c_next = f * c_prev_batch + i * cand_c  # (batch_size, hidden_dim)\n",
        "\n",
        "        # Update hidden state\n",
        "        h_next = o * self.tanh(c_next)\n",
        "\n",
        "        # Cache for backprop\n",
        "        cache = {\n",
        "            'x': x_batch, 'h_prev': h_prev_batch, 'c_prev': c_prev_batch,\n",
        "            'concat': concat, 'f': f, 'i': i, 'cand_c': cand_c,\n",
        "            'o': o, 'c_next': c_next, 'h_next': h_next\n",
        "        }\n",
        "\n",
        "        return c_next, h_next, cache\n",
        "\n",
        "    def forward(self, img_features_batch, cap_seq_batch, train=True):\n",
        "        h = self.tanh(cp.dot(img_features_batch, self.W_h) + self.b_h)\n",
        "        c = self.tanh(cp.dot(img_features_batch, self.W_c) + self.b_c)\n",
        "\n",
        "        batch_size, seq_len = cap_seq_batch.shape\n",
        "        seq_len -= 1\n",
        "\n",
        "        outputs = []\n",
        "        caches = [] if train else None\n",
        "        loss = 0\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            word_indices = cap_seq_batch[:, t]\n",
        "            x = self.embedding_matrix[word_indices]\n",
        "\n",
        "            c, h, cache = self.forward_step(x, h, c)\n",
        "            logits = h @ self.W_out + self.b_out\n",
        "            probs = self.softmax(logits)\n",
        "            outputs.append(probs)\n",
        "\n",
        "            if train:\n",
        "                caches.append(cache)\n",
        "                targets = cap_seq_batch[:, t+1].astype(cp.int32)\n",
        "                batch_indices = cp.arange(batch_size)\n",
        "                PAD_TOKEN_IDX = 0\n",
        "                mask = (cap_seq_batch[:, t] != PAD_TOKEN_IDX).astype(cp.float32)\n",
        "                loss += -cp.sum(cp.log(probs[batch_indices, targets]))\n",
        "\n",
        "        outputs = cp.stack(outputs, axis=1)  # (batch_size, seq_len, vocab_size)\n",
        "\n",
        "        if train:\n",
        "            loss /= (batch_size * seq_len)\n",
        "            return outputs, loss, caches\n",
        "        else:\n",
        "            return outputs\n",
        "\n",
        "    def backward_step(self, dh_next, dc_next, cache):\n",
        "        x = cache[\"x\"]\n",
        "        h_prev = cache['h_prev']\n",
        "        c_prev = cache['c_prev']\n",
        "        concat = cache['concat']\n",
        "        f = cache['f']\n",
        "        i = cache['i']\n",
        "        o = cache['o']\n",
        "        cand_c = cache['cand_c']\n",
        "        h_next = cache['h_next']\n",
        "        c_next = cache['c_next']\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        # Gradient through hidden state\n",
        "        do = dh_next * self.tanh(c_next)\n",
        "        dc_next = dc_next + dh_next * o * (1 - self.tanh(c_next) ** 2)\n",
        "\n",
        "        # Gradient through cell state\n",
        "        dcand_c = dc_next * i\n",
        "        di = dc_next * cand_c\n",
        "        df = dc_next * c_prev\n",
        "        dc_prev = dc_next * f\n",
        "\n",
        "        # Gradient through gates\n",
        "        do_input = do * o * (1 - o)\n",
        "        di_input = di * i * (1 - i)\n",
        "        df_input = df * f * (1 - f)\n",
        "        dcand_c_input = dcand_c * (1- cand_c ** 2)\n",
        "\n",
        "        # Gradient for weights\n",
        "        dW_o = concat.T @ do_input\n",
        "        dW_i = concat.T @ di_input\n",
        "        dW_f = concat.T @ df_input\n",
        "        dW_cand = concat.T @ dcand_c_input\n",
        "\n",
        "        # Gradient for biases\n",
        "        db_o = cp.sum(do_input, axis=0)\n",
        "        db_i = cp.sum(di_input, axis=0)\n",
        "        db_f = cp.sum(df_input, axis=0)\n",
        "        db_cand = cp.sum(dcand_c_input, axis=0)\n",
        "\n",
        "        # Gradient wrt concatanated input\n",
        "        dconcat = (do_input @ self.W_o.T +\n",
        "                   di_input @ self.W_i.T +\n",
        "                   df_input @ self.W_f.T +\n",
        "                   dcand_c_input @ self.W_cand.T)\n",
        "\n",
        "        # Split concatanated gradient\n",
        "        dx = dconcat[:, :self.embedding_dim]\n",
        "        dh_prev = dconcat[:, self.embedding_dim:]\n",
        "\n",
        "        grads = {\n",
        "            'W_o': dW_o, 'b_o': db_o,\n",
        "            'W_i': dW_i, 'b_i': db_i,\n",
        "            'W_f': dW_f, 'b_f': db_f,\n",
        "            'W_cand': dW_cand, 'b_cand': db_cand\n",
        "        }\n",
        "\n",
        "        return dx, dh_prev, dc_prev, grads\n",
        "\n",
        "    def backward(self, img_features_batch, cap_seq_batch, outputs, caches):\n",
        "        batch_size, seq_len = cap_seq_batch.shape\n",
        "        seq_len -= 1\n",
        "\n",
        "        grads = {\n",
        "            'embedding_matrix': cp.zeros_like(self.embedding_matrix),\n",
        "            'W_h': cp.zeros_like(self.W_h),\n",
        "            'b_h': cp.zeros_like(self.b_h),\n",
        "            'W_c': cp.zeros_like(self.W_c),\n",
        "            'b_c': cp.zeros_like(self.b_c),\n",
        "            'W_f': cp.zeros_like(self.W_f),\n",
        "            'b_f': cp.zeros_like(self.b_f),\n",
        "            'W_i': cp.zeros_like(self.W_i),\n",
        "            'b_i': cp.zeros_like(self.b_i),\n",
        "            'W_cand': cp.zeros_like(self.W_cand),\n",
        "            'b_cand': cp.zeros_like(self.b_cand),\n",
        "            'W_o': cp.zeros_like(self.W_o),\n",
        "            'b_o': cp.zeros_like(self.b_o),\n",
        "            'W_out': cp.zeros_like(self.W_out),\n",
        "            'b_out': cp.zeros_like(self.b_out)\n",
        "        }\n",
        "\n",
        "        dh_next = cp.zeros((self.hidden_dim,))\n",
        "        dc_next = cp.zeros((self.hidden_dim,))\n",
        "\n",
        "        # Backpropagate through time\n",
        "        for t in reversed(range(seq_len)):\n",
        "            # Gradients from output layer\n",
        "            dprobs = outputs[:, t, :].copy()\n",
        "            target = cap_seq_batch[:, t+1]\n",
        "            batch_indices = cp.arange(batch_size)\n",
        "            dprobs[batch_indices, target] -= 1\n",
        "            dprobs /= (batch_size * seq_len)\n",
        "\n",
        "            # Gradient from output\n",
        "            h = caches[t][\"h_next\"]\n",
        "            grads[\"W_out\"] += h.T @ dprobs\n",
        "            grads[\"b_out\"] += cp.sum(dprobs, axis=0)\n",
        "            dh = dprobs @ self.W_out.T + dh_next\n",
        "\n",
        "            # Gradient through backprop\n",
        "            dx, dh_next, dc_next, step_grads = self.backward_step(dh, dc_next, caches[t])\n",
        "\n",
        "            # Accumulate gradients\n",
        "            for key in step_grads.keys():\n",
        "                grads[key] += step_grads[key]\n",
        "\n",
        "        # Gradient through initial state projection\n",
        "        h0 = self.tanh(img_features_batch @ self.W_h + self.b_h)\n",
        "        dh0 = dh_next * (1 - h0 ** 2)\n",
        "        grads[\"W_h\"] += img_features_batch.T @ dh0\n",
        "        grads[\"b_h\"] += cp.sum(dh0, axis=0)\n",
        "\n",
        "        c0 = self.tanh(img_features_batch @ self.W_c + self.b_c)\n",
        "        dc0 = dc_next * (1 - c0**2)\n",
        "        grads['W_c'] = img_features_batch.T @ dc0\n",
        "        grads['b_c'] = cp.sum(dc0, axis=0)\n",
        "\n",
        "        # Gradient clipping\n",
        "        for key in grads:\n",
        "            grads[key] = cp.clip(grads[key], -5, 5)\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def update_weights(self, grads):\n",
        "        self.W_f -= self.lr * grads['W_f']\n",
        "        self.b_f -= self.lr * grads['b_f']\n",
        "        self.W_i -= self.lr * grads['W_i']\n",
        "        self.b_i -= self.lr * grads['b_i']\n",
        "        self.W_cand -= self.lr * grads['W_cand']\n",
        "        self.b_cand -= self.lr * grads['b_cand']\n",
        "        self.W_o -= self.lr * grads['W_o']\n",
        "        self.b_o -= self.lr * grads['b_o']\n",
        "        self.W_out -= self.lr * grads['W_out']\n",
        "        self.b_out -= self.lr * grads['b_out']\n",
        "\n",
        "    def update_weights_adam(self, grads):\n",
        "        self.t += 1\n",
        "\n",
        "        for name in grads.keys():\n",
        "            self.m[name] = self.beta_1 * self.m[name] + (1 - self.beta_1) * grads[name]\n",
        "            self.v[name] = self.beta_2 * self.v[name] + (1 - self.beta_2) * grads[name] ** 2\n",
        "\n",
        "            m_hat = self.m[name] / (1 - self.beta_1 ** self.t)\n",
        "            v_hat = self.v[name] / (1 - self.beta_2 ** self.t)\n",
        "\n",
        "            weight = getattr(self, name)\n",
        "            weight -= self.lr * m_hat / (cp.sqrt(v_hat) + self.epsilon)\n",
        "            setattr(self, name, weight)\n",
        "\n",
        "\n",
        "    def train_step(self, img_features_batch, cap_seq_batch):\n",
        "        # Forward pass\n",
        "        outputs, loss, caches = self.forward(img_features_batch, cap_seq_batch)\n",
        "\n",
        "        # Backward pass\n",
        "        grads = self.backward(img_features_batch, cap_seq_batch, outputs, caches)\n",
        "\n",
        "        # Update weights\n",
        "        self.update_weights_adam(grads)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def predict(self, img_features, idx2word, max_len=20, start_idx=1, end_idx=2):\n",
        "        # Initialize states\n",
        "        h = self.tanh(img_features @ self.W_h + self.b_h)\n",
        "        c = self.tanh(img_features @ self.W_c + self.b_c)\n",
        "\n",
        "        caption = []\n",
        "\n",
        "        current_word_idx = start_idx\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            # Get embedding\n",
        "            x = self.embedding_matrix[current_word_idx]\n",
        "\n",
        "            # Add batch dimension for compatibility\n",
        "            x_batch = x[cp.newaxis, :]\n",
        "            h_batch = h[cp.newaxis, :]\n",
        "            c_batch = c[cp.newaxis, :]\n",
        "\n",
        "            # Forward step\n",
        "            c, h, _ = self.forward_step(x_batch, h_batch, c_batch)\n",
        "\n",
        "            # Remove dimensions\n",
        "            h = h.squeeze()\n",
        "            c = c.squeeze()\n",
        "\n",
        "            # Compute logits and probabilities\n",
        "            logits = h @ self.W_out + self.b_out\n",
        "            probs = self.softmax(logits)\n",
        "            # print(probs)\n",
        "            # exit()\n",
        "            # print(probs, probs.shape, type(probs))\n",
        "\n",
        "            # Sample next word\n",
        "            current_word_idx = cp.argmax(probs)\n",
        "            # print(current_word_idx)\n",
        "\n",
        "            if current_word_idx == end_idx:\n",
        "                break\n",
        "\n",
        "            caption.append(idx2word[str(current_word_idx)])\n",
        "\n",
        "        return \" \".join(caption)\n",
        "\n",
        "    def save_model(self, path):\n",
        "        weights = {\n",
        "            'embedding_matrix': cp.asnumpy(self.embedding_matrix),\n",
        "            'W_h': cp.asnumpy(self.W_h),\n",
        "            'b_h': cp.asnumpy(self.b_h),\n",
        "            'W_c': cp.asnumpy(self.W_c),\n",
        "            'b_c': cp.asnumpy(self.b_c),\n",
        "            'W_f': cp.asnumpy(self.W_f),\n",
        "            'b_f': cp.asnumpy(self.b_f),\n",
        "            'W_i': cp.asnumpy(self.W_i),\n",
        "            'b_i': cp.asnumpy(self.b_i),\n",
        "            'W_cand': cp.asnumpy(self.W_cand),\n",
        "            'b_cand': cp.asnumpy(self.b_cand),\n",
        "            'W_o': cp.asnumpy(self.W_o),\n",
        "            'b_o': cp.asnumpy(self.b_o),\n",
        "            'W_out': cp.asnumpy(self.W_out),\n",
        "            'b_out': cp.asnumpy(self.b_out),\n",
        "            'config': {\n",
        "                'vocab_size': self.vocab_size,\n",
        "                'embedding_dim': self.embedding_dim,\n",
        "                'hidden_dim': self.hidden_dim,\n",
        "                'img_dim': self.img_dim\n",
        "            }\n",
        "        }\n",
        "        np.savez(path, **weights)\n",
        "        print(f\"Model saved to {path}\")\n",
        "\n",
        "    def load_model(self, filepath):\n",
        "        data = np.load(filepath, allow_pickle=True)\n",
        "        self.embedding_matrix = cp.array(data['embedding_matrix'])\n",
        "        self.W_h = cp.array(data['W_h'])\n",
        "        self.b_h = cp.array(data['b_h'])\n",
        "        self.W_c = cp.array(data['W_c'])\n",
        "        self.b_c = cp.array(data['b_c'])\n",
        "        self.W_f = cp.array(data['W_f'])\n",
        "        self.b_f = cp.array(data['b_f'])\n",
        "        self.W_i = cp.array(data['W_i'])\n",
        "        self.b_i = cp.array(data['b_i'])\n",
        "        self.W_cand = cp.array(data['W_cand'])\n",
        "        self.b_cand = cp.array(data['b_cand'])\n",
        "        self.W_o = cp.array(data['W_o'])\n",
        "        self.b_o = cp.array(data['b_o'])\n",
        "        self.W_out = cp.array(data['W_out'])\n",
        "        self.b_out = cp.array(data['b_out'])\n",
        "        print(f\"Model loaded from {filepath}\")\n"
      ],
      "metadata": {
        "id": "XQGpkYed-CQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = np.load(\"/content/drive/MyDrive/datasets/flickr8k/flickr8k/embedding_matrix.npy\")\n",
        "embedding_matrix"
      ],
      "metadata": {
        "id": "Qwz0972RhdU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = embedding_matrix.shape[1]"
      ],
      "metadata": {
        "id": "aSpuVQCSe6VV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = embedding_matrix.shape[0]"
      ],
      "metadata": {
        "id": "cdiSi12ae8Iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim, vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qiBR8H16P3v",
        "outputId": "b774dd2d-ac76-4edf-8dd8-56609a061006"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 5032)"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = LSTMDecoder(embedding_dim, 512, 2048, vocab_size, embedding_matrix, lr=0.001)"
      ],
      "metadata": {
        "id": "D5RwLiW0EwAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "cVbiV6-1CI07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cupy as cp\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "\n",
        "NUM_EPOCHS = 10\n",
        "PAD_TOKEN_IDX = 0\n",
        "PRINT_EVERY = 20\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"TRAINING\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "start_time = time.time()\n",
        "\n",
        "count = 5\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n",
        "    if epoch % 3 == 0:\n",
        "        decoder.lr /= 2\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    # Training\n",
        "    for batch_idx, (features_batch, sequences_batch) in enumerate(train_loader):\n",
        "        features_batch = cp.array(features_batch.numpy(), dtype=cp.float32)\n",
        "        sequences_batch = torch.stack(sequences_batch)\n",
        "        sequences_batch = sequences_batch.T\n",
        "        sequences_batch = cp.array(sequences_batch.numpy(), dtype=cp.int32)\n",
        "\n",
        "\n",
        "        # Training step\n",
        "        loss = decoder.train_step(features_batch, sequences_batch)\n",
        "        epoch_loss += loss\n",
        "        num_batches += 1\n",
        "\n",
        "        # Print progress\n",
        "        if (batch_idx + 1) % PRINT_EVERY == 0:\n",
        "            avg_loss = epoch_loss / num_batches\n",
        "            elapsed = time.time() - start_time\n",
        "            print(f\"  Batch {batch_idx+1}/{len(train_loader)} - Loss: {avg_loss:.4f} - Time: {elapsed/60:.1f}min\")\n",
        "\n",
        "    # Calculate average training loss\n",
        "    avg_train_loss = epoch_loss / num_batches\n",
        "\n",
        "    # Validation\n",
        "    print(\"\\n  Evaluating on validation set...\")\n",
        "    val_loss = 0.0\n",
        "    val_batches = 0\n",
        "\n",
        "    for features_batch, sequences_batch in val_loader:\n",
        "        features_batch = cp.array(features_batch.numpy(), dtype=cp.float32)\n",
        "        sequences_batch = torch.stack(sequences_batch)\n",
        "        sequences_batch = sequences_batch.T\n",
        "        sequences_batch = cp.array(sequences_batch.numpy(), dtype=cp.int32)\n",
        "\n",
        "        outputs, loss, _ = decoder.forward(features_batch, sequences_batch, train=True)\n",
        "        val_loss += float(cp.asnumpy(loss))\n",
        "        val_batches += 1\n",
        "\n",
        "    avg_val_loss = val_loss / val_batches\n",
        "\n",
        "    # Print epoch summary\n",
        "    print(f\"\\n  Epoch {epoch + 1} Summary:\")\n",
        "    print(f\"    Train Loss: {avg_train_loss:.4f}\")\n",
        "    print(f\"    Val Loss:   {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Save best model\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        decoder.save_model(f'/content/drive/MyDrive/datasets/flickr8k/flickr8k/saved_models/best_model{count}.npz')\n",
        "        print(f\"  ✓ Best model saved! (Val Loss: {avg_val_loss:.4f})\")\n",
        "        count+=1\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"TRAINING COMPLETE!\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
        "print(f\"Total time: {(time.time() - start_time)/60:.1f} minutes\")"
      ],
      "metadata": {
        "id": "wMH-ad4dfNl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_loader.dataset)"
      ],
      "metadata": {
        "id": "9XaBeYwKnQB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir"
      ],
      "metadata": {
        "id": "qAX4WclS7myO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "bbb26dc7-e573-4753-87e8-d19e544d1d54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/datasets/flickr8k/flickr8k/split'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading saved model for further training\n",
        "decoder = LSTMDecoder(embedding_dim, 512, 2048, vocab_size, embedding_matrix, lr=0.001)\n",
        "decoder.load_model(\"/content/drive/MyDrive/datasets/flickr8k/flickr8k/saved_models/best_model5.npz\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPTQp8Cx6eui",
        "outputId": "8200a306-8b79-4e7d-cf97-616e4cd8f99c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded from /content/drive/MyDrive/datasets/flickr8k/flickr8k/saved_models/best_model5.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing"
      ],
      "metadata": {
        "id": "rrVccY1qNAzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/datasets/flickr8k/flickr8k/idx2word.json\") as f:\n",
        "    idx2word = json.load(f)"
      ],
      "metadata": {
        "id": "_uU7QJOUNmDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_decoder = LSTMDecoder(embedding_dim, 512, 2048, vocab_size, embedding_matrix, lr=0.001)\n",
        "test_decoder.load_model(\"/content/drive/MyDrive/datasets/flickr8k/flickr8k/saved_models/best_model7.npz\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhllKMWRBwSu",
        "outputId": "e23fe156-be0b-4670-c613-7825da91aff9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded from /content/drive/MyDrive/datasets/flickr8k/flickr8k/saved_models/best_model7.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_captions = []\n",
        "START_TOKEN_IDX = 1\n",
        "END_TOKEN_IDX = 2\n",
        "for i, (img, features)  in enumerate(test_features.items()):\n",
        "    # Convert to CuPy\n",
        "    features = cp.array(features, dtype=cp.float32)\n",
        "\n",
        "    # Generate caption\n",
        "    caption = test_decoder.predict(\n",
        "        features,\n",
        "        idx2word\n",
        "    )\n",
        "\n",
        "    generated_captions.append(caption)\n",
        "\n",
        "    # Print progress\n",
        "    if (i + 1) % 100 == 0:\n",
        "        print(f\"Generated {i+1}/{len(test_features)} captions...\")\n",
        "\n",
        "print(f\"✓ Generated all {len(generated_captions)} captions\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SAMPLE GENERATED CAPTIONS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for i in range(10):\n",
        "    print(f\"\\nImage {i+1}:\")\n",
        "    print(f\"  Generated: {generated_captions[i]}\")\n",
        "    print(\"-\" * 70)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdYQHTCMNSJc",
        "outputId": "a496efbf-6c3e-4a55-a852-b2d06daf4bfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 100/810 captions...\n",
            "Generated 200/810 captions...\n",
            "Generated 300/810 captions...\n",
            "Generated 400/810 captions...\n",
            "Generated 500/810 captions...\n",
            "Generated 600/810 captions...\n",
            "Generated 700/810 captions...\n",
            "Generated 800/810 captions...\n",
            "✓ Generated all 810 captions\n",
            "\n",
            "======================================================================\n",
            "SAMPLE GENERATED CAPTIONS\n",
            "======================================================================\n",
            "\n",
            "Image 1:\n",
            "  Generated: a woman in a black shirt is standing on a bench in front of a building\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Image 2:\n",
            "  Generated: a black dog is running through the grass\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Image 3:\n",
            "  Generated: a boy in a red shirt is jumping down a wooden wall\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Image 4:\n",
            "  Generated: a dog is running through a grassy area\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Image 5:\n",
            "  Generated: a person in a boat is standing on a rocky beach\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Image 6:\n",
            "  Generated: a black dog is running through the snow\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Image 7:\n",
            "  Generated: a man in a black shirt is standing on a mountain\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Image 8:\n",
            "  Generated: a man in a black shirt is climbing a rock\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Image 9:\n",
            "  Generated: a girl in a red shirt is standing in a <unk>\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Image 10:\n",
            "  Generated: a person in a red jacket is standing on a mountain\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_caption(image):\n",
        "    features = cp.array(test_features[image], dtype = cp.float32)\n",
        "\n",
        "    caption = test_decoder.predict(\n",
        "        features,\n",
        "        idx2word\n",
        "    )\n",
        "\n",
        "    return caption"
      ],
      "metadata": {
        "id": "wxri6kpGPOf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_caption(\"1019077836_6fc9b15408.jpg\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "eQ_FB0A7Wpnv",
        "outputId": "05727835-d1e3-41e0-c445-5376270971f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a brown dog is running through a field'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1hmL_z71vD_MgoIEkwB6ngj8v8gN2vINx",
      "authorship_tag": "ABX9TyOudCLmUcLpUCvBll7I2aaK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}